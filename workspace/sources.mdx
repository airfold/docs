---
title: 'Source'
description: 'Bring your data into Airfold.'
icon: "database"
---

sources make it easy to bring your data into Airfold. sources are akin to tables in a database. They store data according to a defined schema.

All the workspace data lives inside sources, which can be queried, filtered, aggregated, and joined using SQL.

Data is ingested into sources directly through Airfold's [Ingest API](/ingest) or by a [materialized pipe](/workspace/pipes).

Additionally, sources can be enhanced into AI sources for transforming unstructured data into structured formats, unlocking powerful analytical potential.

### Sample Sources

<CodeGroup>
```yaml source.yaml
description: "Error logs from a web server"
cols:
  ts: DateTime
  file: String
  error: String
settings: |
  MergeTree()
  PARTITION BY toYear(ts)
  ORDER BY (ts, file)
```

```yaml source_ai.yaml
description: "Error logs from a web server"
cols:
  ts: DateTime
  file: String
  error: String
ai_cols:
    - name: error_description
      type: String
      description: Error description
      using:
          - error
          - file
settings: |
  MergeTree()
  PARTITION BY toYear(ts)
  ORDER BY (ts, file)
```
</CodeGroup>

<br />

<Note>
The `ai_cols` field specifies which columns the AI source needs to fill. The `using` field specifies the columns to use in the AI transformation.
</Note>

## The `_errors` Table
Every Workspace has an associated `_errors` source table, which contains all errors that occurred during ingestion, for example, if a row failed to be ingested due to schema mismatch.

This way, the whole ingestion process is not interrupted by a single error, and the error can be reviewed and fixed later.

The `_errors` is treated like any other source, and can be queried, filtered, and joined with other sources, for example, to find the most common errors by source:

```sql
SELECT source, COUNT(*) AS count
FROM _errors
GROUP BY source
ORDER BY count DESC
```

## Push a Source
Push sources to your [workspace](/workspace/workspaces) using the [CLI `af push`](/reference/cli) or the [`/push` API](/api-reference/workspace/push).<br />
For example, to push the source above:
```bash
af push source.yaml
```

## Properties

<ResponseField name="description" type="string">
  A brief overview of the source's content or purpose. Optional.
</ResponseField>
<ResponseField name="cols" type="{name: type}" required>
  Defines the schema of the source as a list of columns where each column name is a key and its data type is the value (see [Data Types](https://clickhouse.com/docs/en/sql-reference/data-types)).
</ResponseField>
<ResponseField name="settings" type="string | {key: value} | array">
  [ClickHouse table configuration](https://clickhouse.com/docs/en/sql-reference/statements/create/table) for the source, which can include `ORDER BY`, `PARTITION BY`, Table Engine, etc. The settings can be a String, a key-value pair, or an array comprising either or both. Optional.
</ResponseField>
<ResponseField name="ai_cols" type="array">
  Columns to be filled by the AI source.
  <Expandable title="AIColumn Properties">
    <ResponseField name="name" type="string" required>
      The name of the column.
    </ResponseField>
    <ResponseField name="type" type="string" required>
      The data type of the column.
    </ResponseField>
    <ResponseField name="description" type="string" required>
      A description of the column (see [Data Types](https://clickhouse.com/docs/en/sql-reference/data-types)).
    </ResponseField>
    <ResponseField name="default_value" type="string">
      The default value of the column.
    </ResponseField>
    <ResponseField name="using" type="array" required>
      The columns to use in the AI transformation.
    </ResponseField>
  </Expandable>
</ResponseField>
